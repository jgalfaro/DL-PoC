{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplementary Material to: \n",
    "# Blueprint for Next Generation Cyber-Physical Resilience using Defense Quantum Machine Learning\n",
    "\n",
    "# Two-train Example\n",
    "\n",
    "# Authors\n",
    "\n",
    "\n",
    "<a href=\"https://carleton.ca/scs/people/michel-barbeau/\">Michel Barbeau</a>\n",
    "\n",
    "<a href=\"http://www-public.imtbs-tsp.eu/~garcia_a/web/\">Joaquin Garcia-Alfaro</a>\n",
    "\n",
    "Version: April 7, 2020\n",
    "\n",
    "We reused code from: <a href=\"https://pennylane.readthedocs.io/en/stable/introduction/measurements.html\">Measurements</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background on Reinforcement Learning\n",
    "\n",
    "At the core of Reinforcement Learning is a state-transition model. Triggered by an action, every transition is from one state to another. Associated with each action, there is a numerical reward.\n",
    "Reinforcement Learning aims to find an action-selection policy that maximizes rewards resulting from a sequence of transitions.\n",
    "\n",
    "The learning entity is called an agent. It has a state set $S=\\{ s_0, s_1,\\ldots,s_{n-1} \\}$ and a set of actions $A=\\{ a_0, a_1,\\ldots,a_{m-1} \\}$. The reward function $r$ has domain $S \\times A$ and co-domain $\\mathbb{R}$.\n",
    "\n",
    "Multiple actions might be available in a given state. At any given time, the policy tells the agent which one to take.\n",
    "\n",
    "The objective of Reinforcement Learning is finding a policy maximizing the return. A policy is a function $P$ with domain $S$ and co-domain $A$. The goal of Reinforcement Learning is to find a good policy w.r.t. a reward function. That is, the goal is obtain the best possible reward. There are several strategies for obtaining such a policy. The greedy strategy aims to maximize the immediate reward, while ignoring the long term reward. The random strategy picks a random action. The greedy and random are comparison references.\n",
    "\n",
    "Consideration of long-term reward is captured by a utility function $Q$, with domain $S \\times A$ and co-domain $\\mathbb{R}$.\n",
    "Given a state $s$ and an action $a$, $Q(s,a)$ returns the utility associated with performing action $a$ in state $s$, taking into account what is going to occur after $a$ is taken.\n",
    "The utility function can be recursively defined as:\n",
    "$$ Q(s,a) = r(s,a) + \\gamma \\max Q(s',a') $$\n",
    "$s'$ and $a'$ represent the next state-action pair.\n",
    "Constant $\\gamma$, in $[0,1]$ is a discounting factor, weighting the long-term  reward versus the short term one.\n",
    "The Reinforcement Learning problem can then be formulated as\n",
    "$$P(s) = \\arg\\max_a Q(s, a).$$\n",
    "This reads as the policy is defined such that in a state $s$ the chosen action maximizes the utility $Q(s, a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from pennylane.optimize import NesterovMomentumOptimizer\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process (MDP)\n",
    "\n",
    "For background explantions on this example, see Section IV of \"Blueprint for Next Generation Cyber-Physical Resilience using Defense Quantum Machine Learning\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the MDP environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, m = 3, 2; # number of states, number of actions\n",
    "take_loop = 0\n",
    "take_bypass = 1\n",
    "# init transition matrix\n",
    "T = np.zeros((n, n, m),dtype=np.int8)\n",
    "T[0][0][take_loop] = 0.5\n",
    "T[0][0][take_bypass] = 0.5\n",
    "T[0][1][take_loop] = 0.5\n",
    "T[0][1][take_bypass] = 0.5\n",
    "# init reward matrix\n",
    "R = np.zeros((n, n, m),dtype=np.int8)\n",
    "R[0][0][take_loop] = 0\n",
    "R[0][0][take_bypass] = 4\n",
    "R[0][1][take_loop] = 0\n",
    "R[0][1][take_bypass] = 2\n",
    "# Terminal states\n",
    "TerminalStates = [1, 2]\n",
    "# generate binary array representation of states\n",
    "states = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "#print(states)\n",
    "# print(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantum Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of a quantum device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of qubits: This number determines the size of the problem, in single data\n",
    "# items, that can be handled by the program.\n",
    "num_qubits = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = qml.device('default.qubit', wires=num_qubits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the circuit for the variational classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer(W):\n",
    "    qml.Rot(W[0, 0], W[0, 1], W[0, 2], wires=0)\n",
    "    qml.Rot(W[1, 0], W[1, 1], W[1, 2], wires=1)\n",
    "    qml.CNOT(wires=[0, 1])\n",
    "    qml.CNOT(wires=[1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "\n",
    "The state encoding used for this example is $s \\rightarrow \\vert s \\rangle$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statepreparation(x):\n",
    "    # map a list of zeros and ones to a quantum state\n",
    "    qml.BasisState(x, wires=[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of the quantum node, i.e., the structure of the variational circuit. Formal parameter \"weights\" is the number of repetitions of the variational circuit layer structure. Formal parameter \"x\" is an input quantum state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@qml.qnode(dev)\n",
    "def circuit(weights, x=None):\n",
    "    statepreparation(x)\n",
    "    for W in weights:\n",
    "        layer(W)\n",
    "    # return expected values \n",
    "    # return [qml.expval.PauliZ(0),qml.expval.PauliZ(1)]\n",
    "    # return [qml.expval.PauliZ(i) for i in range(2)]\n",
    "    # return qml.probs(0)\n",
    "    return qml.expval.PauliZ(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actualize the variational circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def W(var, x=None):\n",
    "    weights = var[0]\n",
    "    bias = var[1]\n",
    "    return circuit(weights, x=x) + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_loss(labels, predictions):\n",
    "    loss = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        loss = loss + (l - p) ** 2\n",
    "\n",
    "    loss = loss / len(labels)\n",
    "    return loss\n",
    "\n",
    "def accuracy(labels, predictions):\n",
    "\n",
    "    loss = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        if abs(l - p) < 1e-5:\n",
    "            loss = loss + 1\n",
    "    loss = loss / len(labels)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def cost(var, X, Y):\n",
    "    predictions = [W(var, x=x) for x in X]\n",
    "    return square_loss(Y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial target output: [0.5 0.5 0.5 0.5]\n"
     ]
    }
   ],
   "source": [
    "X = states\n",
    "Y = np.zeros(len(X))\n",
    "print(\"Initial target output:\", (Y+1)/2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[[ 0.01764052,  0.00400157,  0.00978738],\n",
      "        [ 0.02240893,  0.01867558, -0.00977278]],\n",
      "\n",
      "       [[ 0.00950088, -0.00151357, -0.00103219],\n",
      "        [ 0.00410599,  0.00144044,  0.01454274]],\n",
      "\n",
      "       [[ 0.00761038,  0.00121675,  0.00443863],\n",
      "        [ 0.00333674,  0.01494079, -0.00205158]]]), 0.0)\n"
     ]
    }
   ],
   "source": [
    "# initialize the parameter of the variational circuit to \n",
    "# random values\n",
    "np.random.seed(0)\n",
    "num_layers = 3\n",
    "# create a \"num_layers\" by \"num_qubits\" by three, 3D matrix\n",
    "var_init = (0.01 * np.random.randn(num_layers, num_qubits, 3), 0.0)\n",
    "print(var_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Step-by-step update of the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = NesterovMomentumOptimizer(0.5)\n",
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent choice:  0  Reward:  0\n",
      "[-1]\n",
      "Iter:     1| Accuracy: 0.0000000 \n",
      "Iter:     2| Accuracy: 0.0000000 \n",
      "Iter:     3| Accuracy: 0.0000000 \n",
      "Iter:     4| Accuracy: 0.0000000 \n",
      "Iter:     5| Accuracy: 0.0000000 \n",
      "Iter:     6| Accuracy: 0.0000000 \n",
      "Iter:     7| Accuracy: 0.0000000 \n",
      "Iter:     8| Accuracy: 0.0000000 \n",
      "Iter:     9| Accuracy: 0.0000000 \n",
      "Iter:    10| Accuracy: 0.0000000 \n",
      "Agent choice:  1  Reward:  2\n",
      "[-1]\n",
      "Iter:    11| Accuracy: 0.0000000 \n",
      "Iter:    12| Accuracy: 0.0000000 \n",
      "Iter:    13| Accuracy: 0.0000000 \n",
      "Iter:    14| Accuracy: 0.0000000 \n",
      "Iter:    15| Accuracy: 0.0000000 \n",
      "Iter:    16| Accuracy: 0.0000000 \n",
      "Iter:    17| Accuracy: 0.0000000 \n",
      "Iter:    18| Accuracy: 0.0000000 \n",
      "Iter:    19| Accuracy: 0.0000000 \n",
      "Iter:    20| Accuracy: 0.0000000 \n",
      "Agent choice:  1  Reward:  2\n",
      "[-1]\n",
      "Iter:    21| Accuracy: 0.0000000 \n",
      "Iter:    22| Accuracy: 0.0000000 \n",
      "Iter:    23| Accuracy: 0.0000000 \n",
      "Iter:    24| Accuracy: 0.0000000 \n",
      "Iter:    25| Accuracy: 0.0000000 \n",
      "Iter:    26| Accuracy: 0.0000000 \n",
      "Iter:    27| Accuracy: 0.0000000 \n",
      "Iter:    28| Accuracy: 0.0000000 \n",
      "Iter:    29| Accuracy: 0.0000000 \n",
      "Iter:    30| Accuracy: 0.0000000 \n",
      "Agent choice:  1  Reward:  0\n",
      "[-1]\n",
      "Iter:    31| Accuracy: 0.0000000 \n",
      "Iter:    32| Accuracy: 0.0000000 \n",
      "Iter:    33| Accuracy: 0.0000000 \n",
      "Iter:    34| Accuracy: 0.0000000 \n",
      "Iter:    35| Accuracy: 0.0000000 \n",
      "Iter:    36| Accuracy: 0.0000000 \n",
      "Iter:    37| Accuracy: 0.0000000 \n",
      "Iter:    38| Accuracy: 0.0000000 \n",
      "Iter:    39| Accuracy: 0.0000000 \n",
      "Iter:    40| Accuracy: 0.0000000 \n",
      "Agent choice:  0  Reward:  0\n",
      "[-1]\n",
      "Iter:    41| Accuracy: 0.0000000 \n",
      "Iter:    42| Accuracy: 0.0000000 \n",
      "Iter:    43| Accuracy: 0.0000000 \n",
      "Iter:    44| Accuracy: 0.0000000 \n",
      "Iter:    45| Accuracy: 0.0000000 \n",
      "Iter:    46| Accuracy: 0.0000000 \n",
      "Iter:    47| Accuracy: 0.0000000 \n",
      "Iter:    48| Accuracy: 0.0000000 \n",
      "Iter:    49| Accuracy: 0.0000000 \n",
      "Iter:    50| Accuracy: 0.0000000 \n",
      "Agent choice:  0  Reward:  4\n",
      "[-1]\n",
      "Iter:    51| Accuracy: 0.0000000 \n",
      "Iter:    52| Accuracy: 0.0000000 \n",
      "Iter:    53| Accuracy: 0.0000000 \n",
      "Iter:    54| Accuracy: 0.0000000 \n",
      "Iter:    55| Accuracy: 0.0000000 \n",
      "Iter:    56| Accuracy: 0.0000000 \n",
      "Iter:    57| Accuracy: 0.0000000 \n",
      "Iter:    58| Accuracy: 0.0000000 \n",
      "Iter:    59| Accuracy: 0.0000000 \n",
      "Iter:    60| Accuracy: 0.0000000 \n",
      "Agent choice:  1  Reward:  2\n",
      "[-1]\n",
      "Iter:    61| Accuracy: 0.0000000 \n",
      "Iter:    62| Accuracy: 0.0000000 \n",
      "Iter:    63| Accuracy: 0.0000000 \n",
      "Iter:    64| Accuracy: 0.0000000 \n",
      "Iter:    65| Accuracy: 0.0000000 \n",
      "Iter:    66| Accuracy: 0.0000000 \n",
      "Iter:    67| Accuracy: 0.0000000 \n",
      "Iter:    68| Accuracy: 0.0000000 \n",
      "Iter:    69| Accuracy: 0.0000000 \n",
      "Iter:    70| Accuracy: 0.0000000 \n",
      "Agent choice:  0  Reward:  0\n",
      "[-1]\n",
      "Iter:    71| Accuracy: 0.0000000 \n",
      "Iter:    72| Accuracy: 0.0000000 \n",
      "Iter:    73| Accuracy: 0.0000000 \n",
      "Iter:    74| Accuracy: 0.0000000 \n",
      "Iter:    75| Accuracy: 0.0000000 \n",
      "Iter:    76| Accuracy: 0.0000000 \n",
      "Iter:    77| Accuracy: 0.0000000 \n",
      "Iter:    78| Accuracy: 0.0000000 \n",
      "Iter:    79| Accuracy: 0.0000000 \n",
      "Iter:    80| Accuracy: 0.0000000 \n",
      "Agent choice:  1  Reward:  0\n",
      "[-1]\n",
      "Iter:    81| Accuracy: 0.0000000 \n",
      "Iter:    82| Accuracy: 0.0000000 \n",
      "Iter:    83| Accuracy: 0.0000000 \n",
      "Iter:    84| Accuracy: 0.0000000 \n",
      "Iter:    85| Accuracy: 0.0000000 \n",
      "Iter:    86| Accuracy: 0.0000000 \n",
      "Iter:    87| Accuracy: 0.0000000 \n",
      "Iter:    88| Accuracy: 0.0000000 \n",
      "Iter:    89| Accuracy: 0.0000000 \n",
      "Iter:    90| Accuracy: 0.0000000 \n",
      "Agent choice:  1  Reward:  2\n",
      "[-1]\n",
      "Iter:    91| Accuracy: 0.0000000 \n",
      "Iter:    92| Accuracy: 0.0000000 \n",
      "Iter:    93| Accuracy: 0.0000000 \n",
      "Iter:    94| Accuracy: 0.0000000 \n",
      "Iter:    95| Accuracy: 0.0000000 \n",
      "Iter:    96| Accuracy: 0.0000000 \n",
      "Iter:    97| Accuracy: 0.0000000 \n",
      "Iter:    98| Accuracy: 0.0000000 \n",
      "Iter:    99| Accuracy: 0.0000000 \n",
      "Iter:   100| Accuracy: 0.0000000 \n"
     ]
    }
   ],
   "source": [
    "# probabilties\n",
    "p = 0.5\n",
    "q = 0.5\n",
    "# init reward matrix\n",
    "R = np.zeros((m, m),dtype=np.int8)\n",
    "R[take_loop][0] = 0\n",
    "R[take_loop][1] = 4\n",
    "R[take_bypass][0] = 0\n",
    "R[take_bypass][1]= 2\n",
    "var = var_init\n",
    "# Expected choice\n",
    "E = 0 \n",
    "alpha = 0.5\n",
    "for i in range(10):\n",
    "    # choices made at point zero\n",
    "    # generate a random agent action\n",
    "    a = np.random.randint(0, 2)\n",
    "    # generate at random the corresponding reward\n",
    "    if a==take_loop:\n",
    "        r = np.random.binomial(1, p, 1)\n",
    "    else:\n",
    "        r = np.random.binomial(1, q, 1) \n",
    "    reward = R[a][r[0]]   \n",
    "    print(\"Agent choice: \", a, \" Reward: \", reward)\n",
    "    # V = (1-alpha)*V + \n",
    "\n",
    "    oldY = [E*2-1]\n",
    "    print(oldY)\n",
    "    # update variational circuit\n",
    "    for j in range(10):\n",
    "        var = opt.step(lambda v: cost(v, [X[0]],oldY), var)\n",
    "        # update output, according to estimated reward\n",
    "        newY = [W(var, X[0])]\n",
    "        acc = accuracy(oldY, newY)\n",
    "        print(\"Iter: {:5d}| Accuracy: {:0.7f} \".format((i*10)+j+1, acc))\n",
    "        if abs(1 - acc) < 0.1:\n",
    "            break;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
